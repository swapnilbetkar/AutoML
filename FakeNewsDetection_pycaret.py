# -*- coding: utf-8 -*-
"""2-fakenewsdetection-pycaret.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/swapnilbetkar/python/blob/main/2_fakenewsdetection_pycaret.ipynb

# Importing All Libraries
"""

!pip install transformers
!pip install pycaret

import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import transformers
from transformers import AutoModel, BertTokenizerFast
from sklearn.decomposition import PCA
import tensorflow_hub as hub
from pycaret.classification import * 
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import plot_confusion_matrix
#from googletrans import Translator
from matplotlib import pyplot as plt
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif' 
plt.rcParams['font.serif'] = 'Ubuntu' 
plt.rcParams['font.monospace'] = 'Ubuntu Mono' 
plt.rcParams['font.size'] = 14 
plt.rcParams['axes.labelsize'] = 12 
plt.rcParams['axes.labelweight'] = 'bold' 
plt.rcParams['axes.titlesize'] = 12 
plt.rcParams['xtick.labelsize'] = 12 
plt.rcParams['ytick.labelsize'] = 12 
plt.rcParams['legend.fontsize'] = 12 
plt.rcParams['figure.titlesize'] = 12 
plt.rcParams['image.cmap'] = 'jet' 
plt.rcParams['image.interpolation'] = 'none' 
plt.rcParams['figure.figsize'] = (10, 10
                                 ) 
plt.rcParams['axes.grid']=False
plt.rcParams['lines.linewidth'] = 2 
plt.rcParams['lines.markersize'] = 8
colors = ['xkcd:pale range', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',
'xkcd:scarlet']
bbox_props = dict(boxstyle="round,pad=0.3", fc=colors[0], alpha=.5)
import pandas as pd
import pycaret

"""# Mounting Dataset folder"""

!ln -s /content/drive/MyDrive/al/fake_news_detection .

"""# Loading Dataset
### The real news and the fake ones are reported into two csvs
"""

import os
# true_data = pd.read_csv(os.path.join('/content/fake_newd_detection','True.csv'))
# fake_data = pd.read_csv(os.path.join('/content/fake_newd_detection','Fake.csv'))
from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
true_data = pd.read_csv("/content/fake_news_detection/True.csv")
fake_data = pd.read_csv("/content/fake_news_detection/Fake.csv")

"""# Sneak-Peak into data and format"""

true_data.head()

fake_data.head()

"""### A $Target column$ is added and the data are merged & randomly mixed into a single dataframe known as data."""

true_data['Target']=['True']*len(true_data)
fake_data['Target']=['Fake']*len(fake_data)

data=true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])

label_size = [len(fake_data), len(true_data)]
plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')
plt.title('Data Distribution Pie Chart')

"""### The Target column is made of strings, and it is not computer-friendly. Let’s adjust it:"""

data['label']=pd.get_dummies(data.Target)['Fake']

"""## Universal Sentence Encoder + PyCaret
#### The Universal Sentence Encoder is an Encoder (a special way to transform a sentence in a vector) that has been trained on several classification tasks. This permits to change each instance of the dataset into a 512 dimensional vector
"""

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

"""### Encoding Dataset"""

data_matrix = embed(data.title.tolist())

"""### Train-Test Split"""

train_data = data.loc[0:int(len(data)*0.8)]
test_data = data.loc[int(len(data)*0.8):len(data)]

"""####  PyCaret permits to adopt almost all the most famous and efficient classification algorithms and compare them. Nonetheless, we don’t want to use 512 x 40000+ numbers, so it is wise to perform a Principal Component Analysis (PCA) dimensional reduction. Performing PCA with 3 components"""

pca = PCA(n_components=4)
pca_data = pca.fit(data_matrix[0:len(train_data)])
pca_train = pca.transform(data_matrix[0:len(train_data)])

pca_test = pca.transform(data_matrix[int(len(data)*0.8):len(data)])

pca_3_data = pd.DataFrame({'First Component':pca_train[:,0],'Second Component':pca_train[:,1],'Third Component':pca_train[:,2], 'Fourth Component':pca_train[:,3],'Target': train_data.Target})

pca_3_test = pd.DataFrame({'First Component':pca_test[:,0],'Second Component':pca_test[:,1],'Third Component':pca_test[:,2], 'Fourth Component':pca_test[:,3], 'Target': test_data.Target})

import seaborn as sns

plt.figure(figsize=(20,10))
plt.subplot(1,3,1)
sns.scatterplot(x='First Component', y = 'Second Component',hue='Target',data=pca_3_data,s=2)
plt.grid(True)
plt.subplot(1,3,2)
sns.scatterplot(x='First Component', y = 'Third Component',hue='Target',data=pca_3_data,s=2)
plt.grid(True)
plt.subplot(1,3,3)
sns.scatterplot(x='Second Component', y = 'Third Component',hue='Target',data=pca_3_data,s=2)
plt.grid(True)

"""### Using use PyCaret and its Machine Learning models"""

setup(data = pca_3_data, target='Target')

best_model = compare_models()

"""## the best model is saved as best_model and it is the Extra Trees Classifier. Let’s use it to predict the test set"""

best_model

le = LabelEncoder()
y_true = le.fit_transform(test_data.Target)
y_pred = best_model.predict(pca_3_test[['First Component','Second Component','Third Component','Fourth Component']])
print(classification_report(y_pred,y_true))

plot_confusion_matrix(best_model,pca_test,y_true,cmap='plasma')

from pycaret.classification import *

plot_model(best_model, plot = 'confusion_matrix')

# AUC Plot
plot_model(best_model, plot = 'auc')

